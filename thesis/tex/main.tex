\documentclass[
  twoside,
  11pt, a4paper,
  footinclude=true,
  headinclude=true,
  cleardoublepage=empty
]{scrreprt}

\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage[ngerman,english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{lmodern}
\usepackage{float}
\usepackage{sidecap}
\usepackage{pgfplots}
\usepackage{todonotes}
\usepackage{minted}
\usepackage{siunitx}
\usepackage{acronym}
\usepackage{setspace}
\usepackage[customcolors]{hf-tikz}
\usepackage{url}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\graphicspath{ {images/} }

\begin{document}

\input{frontbackmatter/title.tex}

\chapter*{Abstract}
\onehalfspace
This study aims to investigate the viability of a physically-based technique called
\textbf{path tracing} in lieu of or in corporation with classical techniques in interactive media
such as video games and visual effects tools.

Real time path tracing has been prohibitively expensive in regards to computational complexity.
However, modern \acs{gpu}s and even \acs{cpu}s have finally gotten fast enough for real time path
tracing to become a viable alternative to traditional real time approaches to rendering.  Based on
that assumption, this thesis presents the idea, algorithm and complexity behind path tracing in the
first part and extrapolates feasibility and suitability of real time path tracing on consumer
hardware according to the current state of technology and trends in the second part.

As part of the research, the author has implemented a path tracing 3D engine in modern C++ in order
to empirically test the assumptions made in this thesis. The study found path tracing to be a
viable rendering technique for average commodity hardware in approximately 4 years.
\singlespace

\chapter*{Acknowledgments}
\doublespacing
I would like to express my sincere gratitude to the teachers throughout school and university for
the knowledge they've passed on.
I thank my friends for the laughs, mistakes and triumphs we shared with one another.

Furthermore, none of this would have been possible without the incredible efforts and love of my
parents who have supported me throughout the years and enabled me to live a carefree life until I
was ready to fend for myself.

Lastly, but certainly not least, I would like to declare my gratefulness to Alisa, whose endless love
has given my life a new meaning.

\singlespace

\clearpage
\vspace*{\fill}
\thispagestyle{empty} % suppress showing of page number
\begin{quotation}
    \em
    We all make choices in life, but in the end, our choices make us.

    \medskip
    \raggedleft
    Andrew Ryan
\end{quotation}
\vspace*{\fill}

\tableofcontents

\chapter*{Acronyms}
\begin{acronym}
    \acro{aabb}[AABB]{axis-aligned bounding box}
    \acroplural{aabb}[AABBs]{axis-aligned bounding boxes}
    \acro{ao}[AO]{ambient occlusion}
    \acro{brdf}[BRDF]{bidirectional reflection distribution function}
    \acro{bvh}[BVH]{bounding volume hierarchy}
    \acro{bsp}[BSP]{binary space partitioning}
    \acro{cpu}[CPU]{central processing unit}
    \acroplural{cpu}[CPUs]{central processing units}
    \acro{csg}[CSG]{constructive solid geometry}
    \acro{fps}[FPS]{frames per second}
    \acro{gi}[GI]{global illumination}
    \acro{gpu}[GPU]{graphics processing unit}
    \acro{psnr}[PSNR]{Peak signal-to-noise ratio}
    \acroplural{gpu}[GPUs]{graphics processing units}
    \acro{hdr}[HDR]{high dynamic range}
    \acro{gflops}[GFLOPS]{billions of floating point operations per second}
    \acro{mlt}[MLT]{Metropolis light transport}
    \acro{pbr}[PBR]{physically based rendering}
    \acro{rt}[RT]{real time}
    \acro{rgb}[RGB]{red-green-blue}
    \acro{simd}[SIMD]{single instruction multiple data}
    \acro{spp}[SPP]{samples per pixel}
    \acro{ssao}[SSAO]{screen-space ambient occlusion}
\end{acronym}

\chapter{Introduction}
As part of the quest for ever-improving game graphics, researchers, graphics hardware developers
and video game developers alike have been coming up with more and more convoluted and technically
challenging ways of improving the graphics in interactive media such as games and visualizations in
order to give users a deeper sense of immersion or to provide special effects artists with faster
feedback.

While rendering techniques are currently shifting from the traditional fixed pipeline approach
towards the new, fully programmable approach that lets developers implement deferred renderers that
can more closely mimic reality by using multiple combined shading and lighting algorithms and
rendering the scene multiple times for different buffers, the fundamental concept of
rasterization-based rendering has largely remained the same.

The real world photon-collecting approach that actual cameras use has so far not been adopted for
interactive media by the industry in any capacity because the computational cost has historically
been prohibitively expensive. It is, however, used extensively (and has been in use since decades)
for offline, non-interactive rendering of computer-generated movies and visualizations of
scientific simulations.

This study assumes that the next logical step for the industry will be to adopt this method for
real time media as well. For the purpose of this thesis, a renderer is considered
\textit{real time} when it manages to render a frame within \(16.67ms\) since that equals 60
\ac{fps} which is
the current de facto standard refresh rate for most available computer screens. Conversely, a
renderer is called \textit{offline} when it is not designed for interactive rendering which usually
means that it will renderer an image or a batch of images over the course of a few days. The
differences of real time and offline path tracing renderers will be explained in the next chapter.

The focus of this research is, first and foremost, interactivity. Therefore, whenever a trade-off between
interactivity and image quality is considered, we will always prefer rendering speed if the target
of 60 \ac{fps} would otherwise not be reached anymore.

\section{Motivation}
Real time path tracing (and physically based rendering in general) offers many
benefits over traditional real time rendering methods such as better visuals
and simpler implementation but also allows for completely new types of graphics
such as realistic caustics \cite{wiki:caustics} and even light dispersion
\cite{wiki:dispersion} (using a prism, for instance) since path tracers might simulate wavelengths
instead of plain \ac{rgb} colors. Modern video games tend
to rely on a growing number of tricks to keep them visually appealing as the
consumer grows more demanding. They're called \textit{tricks} in this study
because they merely trick the beholder into seeing something that appears to be
physically accurate when it is, in fact, not the result of a physically-based
calculation and as such this study aims to keep tricks and emergent phenomena
separated by language. Some notable tricks include \ac{ssao}
\cite{wiki:ssao}, motion blur \cite{wiki:motion-blur}, lens flares
\cite{wiki:lens-flare}, chromatic aberration \cite{wiki:chromatic-aberration},
depth of field \cite{wiki:depth-of-field} and light mapping \cite{wiki:lightmap}.

\section{Leading Questions and Goals}
The primary research objective of this thesis is finding out when real time path tracing will be a
viable alternative to rasterization on commodity hardware. This question is explored by looking at
theoretical indicators (such as Moore's Law) and practical indicators (by implementing a real time
path tracer and benchmarking it) about its performance and the current state of the industry.

\chapter{Real Time Path Tracing Explained}
This chapter will explain the concepts, mathematics, physics and algorithms behind path tracing,
how real time path tracing differs from offline path tracing and the trade-offs made to achieve
acceptable performance. It will also explain how path tracing differs from rasterization and other
\ac{gi} techniques.

\section{Physically Based Approach}
In our physical world, we see pictures because our eyes collect photons emitted by light
sources which then bounce around various surfaces until they eventually hit our eyes'
photoreceptor cells. On every bounce, a bit of light is absorbed which is why light loses intensity
when it bounces. Some surfaces absorb a particular band of wavelengths of the light when it bounces
which we perceive as a change in the light's color. Cameras work exactly like this as far as
collection of photons is concerned.

This physical approach would be extremely wasteful and
computationally complex to simulate, however, since most photons never reach an observer. Consider,
for instance, that only an extremely small percentage of all the photons sent by the Sun actually
reach Earth and an even smaller percentage of those are ever observed (although photons
don't have to be observed to have a physical effect, of course). Since we only care for photons that
are relevant to the image that we are trying to render, it makes more sense to use
\emph{backwards ray tracing} in which rays (which simulate streams of photons) are shot from the
observer into the scene for every sensor. It is called \textit{backwards} because the rays go
the reverse direction compared to their physical counterparts.

This is efficient since we usually only care about a
single observer (the scene camera) for which we will trace every single ray that it can possibly
perceive. In computer graphics terms, we will trace a ray for every pixel of the camera (and for
now we will assume that the viewport is exactly the same resolution as the camera simplicity's
sake). For every ray, we check for intersections with geometry and then either bounce a few more
times or shoot directly towards a light. We might do this multiple times per pixel to improve image
quality. This is called \emph{sampling}. The more iterations we spend on sampling, the better the
quality of our image becomes. This is called \emph{converging}.
There are a few approaches that improve on this such as bidirectional path tracing
\cite{techreport:pbr} and the \ac{mlt} \cite{inproceedings:metropolis}.

\section{Theoretical Basis}
\subsection{The Rendering Equation}
The fundamental problem solved by path tracing is the \emph{rendering equation} originally
described by James Kajiya \cite{inproceedings:pathtracing}. This thesis uses the form from
Wikipedia \cite{wiki:rendering-equation} since the author considers it easier to read:

\[
        L_{\text{o}}(\mathbf x,\, \omega_{\text{o}},\, \lambda,\, t) \,=
        \, L_{\text{e}}(\mathbf x,\, \omega_{\text{o}},\, \lambda,\, t) \ +
        \, \int_\Omega f_{\text{r}}(\mathbf x,\, \omega_{\text{i}},\ \omega_{\text{o}},\, \lambda,\, t)
        \, L_{\text{i}}(\mathbf x,\, \omega_{\text{i}},\, \lambda,\, t)\,
        (\omega_{\text{i}}\,\cdot\,\mathbf n)\, \operatorname d \omega_{\text{i}}
\]

For our purposes, this can be simplified by removing the time and wavelength components which we
will not make use of:

\[
        L_{\text{o}}(\mathbf x,\, \omega_{\text{o}}) \,=
        \, L_{\text{e}}(\mathbf x,\, \omega_{\text{o}}) \ +
        \, \int_\Omega f_{\text{r}}(\mathbf x,\, \omega_{\text{i}},\ \omega_{\text{o}})
        \, L_{\text{i}}(\mathbf x,\, \omega_{\text{i}})\,
        (\omega_{\text{i}}\,\cdot\,\mathbf n)\, \operatorname d \omega_{\text{i}}
\]

This equation can be broken down into its individual parts to make it easier to explain and
understand:

\[
        \tikzmarkin[set fill color=red!50!brown!30,set border color=red!40!black]{outgoing}
            L_{\text{o}}(\mathbf x,\, \omega_{\text{o}})
        \tikzmarkend{outgoing}
        \, = \,
        \tikzmarkin[set fill color=cyan!70!lime!30,set border color=cyan!40!black]{emitted}
        L_{\text{e}}(\mathbf x,\, \omega_{\text{o}})
        \tikzmarkend{emitted}
        \ + \,
        \tikzmarkin[set fill color=yellow!50!lime!30,set border color=yellow!40!black]{integral}(0.1,-0.4)(-0.1,0.6)
            \int_\Omega
        \tikzmarkin[set fill color=green!50!lime!30,set border color=green!40!black]{brdf}(0.0,-0.2)(-0.0,0.4)
            f_{\text{r}}(\mathbf x,\, \omega_{\text{i}},\ \omega_{\text{o}})
        \tikzmarkend{brdf}
        \,
        \tikzmarkin[set fill color=blue!50!lime!30,set border color=blue!40!black]{incoming}(0.0,-0.2)(-0.0,0.4)
            L_{\text{i}}(\mathbf x,\, \omega_{\text{i}})
        \tikzmarkend{incoming}
        \,
        \tikzmarkin[set fill color=magenta!100!lime!30,set border color=pink!40!black]{attenuation}(0.0,-0.2)(-0.0,0.4)
            (\omega_{\text{i}}\,\cdot\,\mathbf n)
        \tikzmarkend{attenuation}
        \,
            \operatorname d \omega_{\text{i}}
        \tikzmarkend{integral}
\]

\noindent
\(
        \tikzmarkin[set fill color=red!50!brown!30,set border color=red!40!black]{outgoing'}(0.1,-0.15)(-0.1,0.35)
        L_{\text{o}}(\mathbf x,\, \omega_{\text{o}})
        \tikzmarkend{outgoing'}
\)\, is the \textbf{outgoing light} with \(\mathbf x\) being a point on a surface from which the light is
reflected from into direction \(\omega_{\text{o}}\).

\noindent
\(
        \tikzmarkin[set fill color=cyan!70!lime!30,set border color=cyan!40!black]{emitted'}(0.1,-0.15)(-0.1,0.35)
        L_{\text{e}}(\mathbf x,\, \omega_{\text{o}})
        \tikzmarkend{emitted'}
\)\, is the \textbf{emitted light} from point \(\mathbf x\). Usually surfaces don't emit light themselves unless
they are area lights.

\noindent
\(
        \tikzmarkin[set fill color=yellow!50!lime!30,set border color=yellow!40!black]{integral'}(0.1,-0.2)(-0.1,0.35)
        \int_\Omega \, \ldots \, \operatorname d \omega_{\text{i}}
        \tikzmarkend{integral'}
\)\, is the integral over \(\Omega\) which is the hemisphere at \(\mathbf x\) (and thusly
centered around \(\mathbf n\)). All possible values for \(\omega_{\text{i}}\) are therefore
contained in \(\Omega\).

\noindent
\(
        \tikzmarkin[set fill color=green!50!lime!30,set border color=green!40!black]{brdf'}(0.1,-0.15)(-0.1,0.35)
            f_{\text{r}}(\mathbf x,\, \omega_{\text{i}},\ \omega_{\text{o}})
        \tikzmarkend{brdf'}
\)\, is the \textbf{\ac{brdf}} which determines how much light is reflected from
\(\omega_{\text{i}}\) to \(\omega_{\text{o}}\) at \(\mathbf x\).

\noindent
\(
        \tikzmarkin[set fill color=blue!50!lime!30,set border color=blue!40!black]{incoming'}(0.1,-0.15)(-0.1,0.35)
            L_{\text{i}}(\mathbf x,\, \omega_{\text{i}})
        \tikzmarkend{incoming'}
\)\, is the \textbf{incoming light} at \(\mathbf x\) from \(\omega_{\text{o}}\). It is not necessarily
\emph{direct light}. The rendering equation also considers \emph{indirect light} which is light that has already
been reflected.

\noindent
\(
        \tikzmarkin[set fill color=magenta!100!lime!30,set border color=pink!40!black]{attenuation'}(0.1,-0.15)(-0.1,0.35)
            (\omega_{\text{i}}\,\cdot\,\mathbf n)
        \tikzmarkend{attenuation'}
\)\, is the \textbf{normal attenuation} at \(\mathbf x\). The incoming light \(\omega_{\text{i}}\) is
weakened depending on the cosine of the angle between \(\omega_{\text{i}}\) and the surface normal
\(\mathbf n\).
\\

Path tracing offers a numerical solution to the integral found in this equation. For every pixel,
every bounce and every sample of the camera, the rendering equation is solved. It becomes apparent
why this is an expensive algorithm to run. For practical reasons, not every possible value for
\(\Omega\) is sampled since this would take a vast amount of time to calculate at physical photon
density. Instead, only a few possible values for \(\Omega\) are calculated each bounce. Depending
on the exact algorithm used, usually only a low number of samples (approximately 20) is required for the
image to converge to an acceptable level of quality.

\subsection{Algorithm}
The general naive algorithm for path tracing in Python-like pseudocode for diffuse and
emissive materials can be written as follows:

\begin{minted}{python}
max_depth = 5

def trace_ray(ray, depth):
    if depth >= max_depth:
        # Return black since we haven't hit anything but we're
        # at our limit for bounces
        return RGB(0, 0, 0) 

    collision = ray.check_collision()
    if not collision:
        # If we haven't hit anything, we can't bounce again so
        # we return black
        return RGB(0, 0, 0)

    material = collision.material;
    emittance = material.emittance

    # shoot a ray into random direction and recurse
    next_ray = Ray()
    next_ray.origin = collision.position
    next_ray.direction = random_vector_on_hemisphere(collision.normal)

    reflectance_theta = dot(next_ray.direction, collision.normal)
    brdf = 2 * material.reflectance * reflectance_theta
    reflected = trace_ray(next_ray, depth + 1)

    return emittance + (brdf * reflecte)

for sample in range(samples):
    for pixel in pixels:
        trace_path(ray_from_pixel(pixel), 0)
\end{minted}

The algorithmic complexity of this algorithm is not immediately obvious because of its pseudocode
character and a few methods whose implementations are as of yet unknown. It is known, however, that
the output image \(O\) will be a 2D matrix with dimensions given by width \(w\) and height \(h\).
Additionally, for every pixel, multiple samples \(s\) are required in order for the algorithm to
converge to an image of acceptable quality. Every ray has a certain maximum depth \(d\).

So far, this lets us determine the total worst number of rays required per image. In the worst
case, no ray exits early. The resulting formula is:

\[ O_w \times O_h \times s \times d \]

Running this for a single pixel with 30 samples and a ray depth of 5 would require
\[30 \times 5 = \num{150}\]
rays per pixel.

For example, an image with a resolution of \(1920 \times 1080\)
would have \[1920 \times 1080 \times 30 \times 5 \approx \num{300000000}\] rays per frame.

This does still not yield the algorithmic complexity, however, since there is no input size \(n\)
in it. We will assume the lookup time for every ray to be \(O(\log n)\) in a suitable data
structure. Hence, the formula becomes:

\[ O_w \times O_h \times s \times d \times \log n \]

The result of this formula yields the number of total lookups per image.
Continuing the example from above given a scene with 2 million triangles thus results in
\[1920 \times 1080 \times 30 \times 5 \times \log \num{2000000} \approx \num{2000000000}\]
lookups per frame.

In the context of this thesis, a goal was set to reach 60 \ac{fps} which means that
\[1920 \times 1080 \times 30 \times 5 \times \log \num{2000000} \times 60 \approx \num{120000000000}\]
lookups per second.

Thankfully, the whole formula can be reduced to just the lookup time \(O(\log n)\) because the other
formula elements are covariants.

\subsection{Acceleration Data Structures}
The data structure used for the underlying scene is the principal factor for the performance of the
path tracing algorithm. The most commonly used naive data structure for path tracing is a simple
list of shapes. Upon scene lookup, a ray is tested for intersection with every shape. The
complexity in this case is \(O(n)\) where \(n\) is the number of shapes which is comparable to rasterization but a lot worse than it
could be with a proper acceleration data structure.

Assuming a flat list of shapes as the current data structure, the next logical step to improve
scene look up time is to add \acp{aabb} around clusters of smaller shapes. For instance, an
\ac{aabb} might surround an icosahedron shape that is made up of hundreds of triangles. This way,
the triangles inside the \ac{aabb} are only tested for ray intersections if the ray intersects the
\ac{aabb} that surrounds the shape. The complexity is now \(O(m \dot n)\) where \(m\) is the number
of shapes (which is equal to the number of \acp{aabb}) and \(n\) is the number of triangles. This
might seem like a worse complexity class compared to before but in practice the number of
comparisons is a lot lower.

The next iteration on top of simple \acp{aabb} is provided by tree-based acceleration data
structures. The most commonly used ones are the \ac{bvh} and the kd-tree. By using these data
structures, scene lookup performance per ray could be drastically improved to \(O(\log n)\) at the
cost of a tree rebuild once per frame. This is usually a good trade-off since a scene is looked up
millions of times per frame but the tree only has to be rebuilt once. While there are multiple
ways to build a \ac{bvh}, most of them have a complexity close to \(O(n(\log n)^2)\) while building
a kd-tree can be done in \(O(n\log n)\).

\section{Properties of Path Tracing}
This section summarises the general properties of path tracing.

\subsection{Computational Cost}
Even though path tracing has a high initial cost, the lookup time for ray collisions is in
\(O(\log n)\) which means that as scenes increase in complexity, the time spent on doing the
lookups is fairly small. The initial cost of path tracing heavily depends upon screen resolution
and desired quality. Due to the high initial cost, path tracing is generally considered slow and
it made real time path tracing infeasible up until recent years.

\subsection{Dynamic Scenes}
Path tracing is well suited for dynamic scenes since it doesn't depend on pre-computations. This
makes it viable for use in interactive applications. The scene data structure must allow for
dynamic scenes in this case, though.

\subsection{Global Illumination}
Commonly \acf{gi} means that every object's illumination affects every other object and that the
renderer doesn't make a distinction between reflected light and light sources.

\subsection{Problems}
Due to the unbiased and random way rays are reflected from surfaces, it takes a long time for
classic path tracing to
produce sharp caustics as rays tend to very rarely hit objects with caustic properties. This makes
it especially difficult for real time path tracing to produce sharp caustics. This can be
alleviated by using bidirectional path tracing or by using an additional photon map.

Another problem is that subsurface scattering and can't be addressed by classic Monte Carlo path
tracers. This can be addressed by using \emph{volumetric path tracing}.
\cite{wiki:volumetric-path-tracing} \cite{incollection:volumetric-path-tracing}

Since path tracers do not simulate light wavelengths, natural phenomena caused by chromatic
aberration, fluorescence and iridescence can not be realistically simulated. A fairly new
improvement to path tracing called \emph{spectral path tracing} with realistic lenses can produce
physically accurate images in those cases. \cite{inproceedings:realistic-lenses} \cite{site:lambda}
\cite{site:luculentus} \cite{site:robigo-luculenta}

\subsection{Comparison to Traditional Ray Tracing}
Ray tracing is the fundamental algorithm behind path tracing. The only difference is that when a
ray hits an object, it doesn't keep bouncing but instead fires off one ray to every light source
directly. This subtle difference means that ray tracing can only calculate direct lighting as
opposed to \ac{gi}.

\subsection{Comparison to Rasterization}
Rasterization is widely implemented in the industry and most interactive 3D applications use it to
render their scene. Its fundamental algorithm has a complexity of \(O(n)\) and is therefore
theoretically slower than path tracing. However, a wide array of algorithmic improvements such as
back-face culling exist and additionally its initial cost is very low.

Unlike path tracing, it does not automatically simulate a wide range range of physical phenomena.
Physical effects such as shadows, global illumination and caustics have to be calculated separately
in other algorithms and then be composited on top of the rasterized scene. This makes rasterization
complicated to use for when many physical effects are desired.

\subsection{Comparison to Other Global Illumination Algorithms}
This section compares some of the more popular \ac{gi} algorithms beside ray tracing and
path tracing. In general, \ac{gi} is considered to be a group of algorithms that calculate
direct light as well as indirect light for computer graphics scenes. However, not all algorithms
that fulfill this purpose are in fact physically accurate. We will therefore take a look at how
some of these algorithms compare to path tracing.

The algorithms that are compared to path tracing in this section are: \emph{photon mapping},
\emph{radiosity} and
\emph{ambient occlusion}. These were chosen due to their widespread use and their varied approaches.
Other \ac{gi} algorithms include: Lightcuts (and its variants), Point Based Global Illumination
and Spherical harmonic lighting.

To note: This thesis considers path tracing at the current state of research which means that path
tracing, bidirectional path tracing and the \ac{mlt} are shortened to just \emph{path tracing} and
will therefore not be individually compared.

\subsubsection{Photon Mapping}
Photon mapping is a two-step process that was developed in 1996 by Henrik Wann Jensen \cite{inproceedings:photon-mapping} as
an approximate way to simulate charged particles (\emph{photons}) traversing the scene.

In the first step, every photon carries a \emph{charge} and is traced through the scene. On every
collision with scene geometry, it is stored to the \emph{photon map} at that location. Afterwards,
the photon is either reflected, refracted, scattered or absorbed depending on the material and
loses a bit of its charge. The photon map serves as a cache for the second step in which a ray
tracing-like process is used to calculate the radiance of the resulting image.

Compared to path tracing, photon mapping has a few advantages and a few disadvantages. In
particular, photon mapping can simulate subsurface scattering and volume caustics which path
tracing can't accurately calculate. On the other hand, photon mapping is unsuitable for real time
applications with dynamic scenes since the photon map can only be used as long as the scene
geometry or light position doesn't change. In the case the scene geometry changes, the cache is invalidated and a new
photon map has to be calculated which is a slow process.

\subsubsection{Radiosity}
Made originally for simulating heat transfer in 1984 by Goral et al.\cite{article:radiosity},
radiosity is one of the oldest algorithms for calculating \ac{gi}. It outputs a light value for
every patch (a smaller part of a surface) on a cache or map. It is a physically accurate way of
simulating light transfer but cannot simulate volume scattering, fog, caustics, transparent
objects or mirrors. These limitations make it unsuitable to use in complex modern scenes.
Additionally, the cache is invalidated whenever the scene changes and therefore it is also
usually not usable for dynamic scenes.

\subsubsection{Ambient Occlusion}
The idea for \ac{ao} was first presented by Gavin Miller in 1994
\cite{inproceedings:ambient-occlusion}. It is meant as an algorithm to calculate realistic
occlusion of every point in a scene and cannot generate an accurate image on its own. It is
usually used with a classic rasterization renderer whose output image is multiplied with the result
of the \ac{ao} and its resulting image is multiplied. The output of this algorithm is sometimes
called the \emph{ambient occlusion map} which serves as a cache. As such, rendering is extremely
fast once the cache has been calculated. However, this cache is invalidated once the scene changes
and is the algorithm is therefore unsuitable for dynamic scenes.

For real time applications, a variant of \ac{ao} called \acf{ssao} is usually used. While \ac{ssao}
is inaccurate from a physical point of view, it results in some very fast and acceptable
approximations that are suitable for real time applications.

\section{History of Path Tracing}
As with so many things in computer science and science in general, the modern idea of physically
based rendering using path tracing builds upon many important past discoveries and algorithms
such as ray tracing and ray casting.
Arthur Appel is generally credited as being the father of
\emph{ray casting} as he was the first to describe the algorithm in a 1968 paper
\cite{inproceedings:raycasting}.

Ray casting is an important idea needed for \emph{ray tracing} which was
first published in a paper in 1980 by Turner Whitted \cite{article:raytracing}.

\begin{figure}[H]
    \includegraphics{early-raytracing-whitted}
    \centering
    \caption{Turner Whitted's original 1980 \cite{article:raytracing} image showing off the usage of ray tracing for
    reflection, refraction and shadows.}
    \label{fig:early-raytracing-whitted}
\end{figure}
Building upon ray tracing, an improved algorithm was published in 1986 by James T. Kajiya which used ray tracing
combined with a Monte Carlo algorithm in order to create a new algorithm that was called \emph{Monte
Carlo ray tracing} \cite{inproceedings:pathtracing}. Nowadays, Monte Carlo ray tracing is better
known as \emph{path tracing}.

It took another decade for path tracing to become the
physically based rendering approach that it is known for today. In 1996, Eric Lafortune improved
the algorithm by suggesting the usage of bidirectional path tracing \cite{techreport:pbr} and
finally the \ac{mlt} was suggested in 1997 by Eric Veach and Leonidas J. Guibas \cite{inproceedings:metropolis} to
improve performance in complex scenes.

This was the last notable improvement to the algorithm,
though many micro optimizations have since been published. All of these achievements and
improvements are generally collapsed into the term \emph{path tracing} since they do not diverge
from the general algorithm but instead improve upon it.

It took longer still for the industry to become interested in path tracing. The early interest in
ray tracing was of mostly academical and recreational nature. One of the most notable creations of
the early days of ray tracing is The Juggler created and published by Eric Graham in 1986
\cite{site:juggler} on an Amiga 1000. It was a pre-rendered animation using ray tracing. Eric Graham
stated that it took the Amiga 1 hour to render each frame \cite{site:juggler}.

\begin{figure}[H]
    \includegraphics[scale=0.5]{amiga-juggler}
    \centering
    \caption{Eric Graham's Juggler}
    \label{fig:amiga-juggler}
\end{figure}
While the animation seems very primitive compared to the animations of today, it was exceptional at
the time. Ernie Wright's statement about The Juggler provides some contemporary context:

\blockquote[\cite{site:juggler}]{Turner Whitted's paper (1980) is widely regarded as the first modern
description of ray tracing methods in computer graphics. This paper's famous image of balls floating
above a checkerboard floor took 74 minutes to render on a DEC VAX 11/780 mainframe, a \$400,000
computer. The Juggler would appear a mere six years later, created and displayed on a \$2000 Amiga.}

The first feature-length computer-animated film,
\textit{Toy Story}, released in 1995 \cite{wiki:toy-story}, is sometimes miscredited as being the
first film using a ray tracing-like algorithm. However, it actually used traditional scanline
rendering. The first feature-length film using ray tracing, \textit{Cars}, was released much later,
in 2006 \cite{wiki:cars} \cite{inproceedings:cars} and started a wave of interest in the movie industry.

The first example of \emph{real time} path tracing was likely produced by the demo scene
\cite{wiki:demoscene} which was quick to adopt it \cite{site:realtime-radiosity-demos}
for the purpose of producing complex graphics rendered and generated on the fly. One notable
example of this is the WebGL Path Tracing by Evan Wallace made in 2010
\cite{site:webgl-path-tracing} which runs in most modern web browsers, making path tracing very
accessible.

\begin{figure}[H]
    \includegraphics[scale=0.4]{webgl-pathtracer}
    \centering
    \caption{WebGL Path Tracer by Evan Wallace}
    \label{fig:webgl-pathtracer}
\end{figure}

Another example is the demo \textit{5 faces} by Fairlight from 2013
\cite{wiki:5faces-fairlight} which
uses a real time ray tracer running on the \acs{gpu} to render a complex scene at 30 \acs{fps}.

\begin{figure}[H]
    \includegraphics[scale=0.5]{5faces}
    \centering
    \caption{5 faces by Fairlight}
    \label{fig:5faces}
\end{figure}

In the past, some critics have offered critical insights about why it might not be a viable
alternative to rasterization on consumer hardware in the short term
\cite{site:raytracing-vs-rasterization} \cite{site:codinghorror-raytracing}. Even John Carmack of
id Software was sceptical of real time ray tracing in games in 2012 in a comment on Ars Technica
\cite{site:carmack-scepticism}.

\section{Current State of Technology}
Recently the interested in real time path tracing seems to be at peak levels. The most notable
example of this is Jacco Bikker's and Jeroen van Schijndel's \emph{Brigade Renderer} \cite{article:brigade}
\cite{site:brigade}. This renderer is aimed at game developers and is meant to run on commodity
hardware in real time. Its company, OTOY, is also developing a cloud-based rendering solution
called Octane Render \cite{site:octane} for animation professionals.

Microsoft's DirectX 12 \cite{site:dx12-raytracing} is receiving \emph{Hybrid Ray-Traced Shadows}, a
technology that combines real time ray tracing with rasterization to create fast high quality
shadows.

A video game using actual real time path tracing and physics called \emph{Sfera} was created by
David Bucciarelli \cite{site:sfera} in 2011. It uses OpenCL for calculating the paths and OpenGL to
render them to the screen.

It's important to keep in mind that while the real time graphics industry has mostly been driven by
video games, the most important hardware currently exist in game consoles which generally evolve at
a much slower pace than desktop computers in terms of hardware power and their graphics hardware
especially is usually non-upgradeable. This means that it wouldn't be economically viable to
develop a real time path tracing renderer that only ran on current generation desktop computers
because most consumers would not be able to benefit from the technology.

\chapter{Research}
The entirety of the research in this thesis was done with a Monte Carlo path tracer called trac0r.
An effort was made to find an existing open source implementation of a path tracer focused on
real time applications but none seem to exist as of this writing. The most likely cause for that is
the enormous hardware cost currently still associated with doing real time path tracing.

\section{Implementation}
Alongside this thesis, a Monte Carlo Path path tracer called \textit{trac0r} was implemented in C++. It can optionally
make use of OpenCL or OpenMP in order to speed up computation. A conscious decision was made to
only implement triangles for geometric primitives due to two reasons. Firstly, since every
other geometric shape can be expressed easily using an amount of triangles, it seems unnecessary to
add additional complexity (although using triangles to approximate other shapes will be less
efficient from a performance perspective). Secondly, real world applications very rarely make use of perfect
mathematical shapes such as spheres, boxes and tori when rendering. Most modern mesh modeling
approximates these shapes using triangles. Since this renderer is supposed to be used for real
world application such as games and interactive visualizations, it didn't seem necessary to
implement anything in addition to what mesh modeling tools will export. 

Additionally to triangles, \acp{aabb} were implemented with the aim to speed up rendering. While
\textit{trac0r} does not implement any advanced acceleration structures (such as BVHs or kd-trees),
it uses \acp{aabb} to
dramatically decrease the number of triangle intersection tests. Every ray first tests for
intersections with every \ac{aabb} in the scene and upon a hit tests every triangle enclosed by the
\ac{aabb}. The fast Möller–Trumbore intersection algorithm \cite{inproceedings:moller2005fast} is used for
triangle intersection tests.

The program does not currently support importing of externally defined scenes or meshes. Scenes are
defined in-code. Utilities for generating planes, boxes and icosahedra out of triangles are
provided. A properly sized \ac{aabb} is built around every shape.

It uses classic reverse path tracing and implements four basic materials:
\begin{enumerate}
    \item Emitter: Arguably the most important material as it is the one providing the light to a
        scene. This simulates a black body heated to a certain temperature and as such serves as an
        area light when applied to geometry. Any ray hitting geometry using the emitter material is
        terminated. A scene without any geometry using emitter material will be utterly dark.
    \item Diffuse: A matte material that scatters light to a random location on the hemisphere
        around the normal of the intersected geometry.
    \item Glass: A material that will choose between reflection and refraction depending on the
        Fresnel coefficients \cite{wiki:fresnel}. It is the only material capable of handling
        intersections within an object.
    \item Glossy: A metal-like material that will reflect a ray towards a cone centered around the
        direction of the outgoing light \(\omega_\text{o}\). A roughness parameter determines 
        the opening angle of that cone. A roughness of 0 will result in a perfect mirror while a roughness
        of 1 will produce a cone with an angle of \(\pi\).
\end{enumerate}

Russian roulette is used as a ray terminator with \[\text{continuation\_probability} = 1 - \frac 1
{\text{max\_depth} - \text{depth}}.\]

Rays are randomly offset within a pixel which results in anti-aliasing. For generating random
numbers, it uses \texttt{xorshift*} generators.

A secondary program was implemented additionally to the renderer called \textit{trac0r\_viewer}
which allows for interactive scene navigation.

\section{Results}
\begin{figure}[H]
    \includegraphics[scale=0.5]{trac0r_viewer1.png}
    \centering
    \caption{trac0r\_viewer viewport after 2 seconds of sampling}
    \label{fig:trac0r_viewer1}
\end{figure}

\begin{figure}[H]
    \includegraphics[scale=0.5]{trac0r_viewer2.png}
    \centering
    \caption{trac0r\_viewer viewport after 30 seconds of sampling}
    \label{fig:trac0r_viewer2}
\end{figure}

Comparing \ac{fps} is not a particularly good criterion between different path tracing since
usually \ac{fps} doesn't correlate to rate of convergence. That is, one path tracer might be able
to iterate at 60 \ac{fps} but takes three seconds to converge while a different path tracer may
only be able to do 10 \ac{fps} but takes only 2 seconds to converge. This might seem unintuitive at
first but remember that path tracing is a very different algorithm compared to rasterization. In
rasterization, once a frame is rendered, the image is "done", it won't improve by rendering the
same scene a second time. In path tracing, particularly Monte Carlo path tracing, an image will be
improved with every new frame that is rendered using the same parameters due to how Monte Carlo
integration works.

In spite of that, this thesis uses \ac{fps} as the primary criterion for two reasons. Firstly, we are not
comparing different path tracers to the one made as part of the thesis. We only compare it
against itself in different scene and hardware configurations. Secondly, \ac{fps} is much easier
to work with and compare than image quality. One might use \ac{psnr} to do the latter but that
still leaves open the question of when to measure the image as even a single frame might take
longer than a second to render under specific circumstances.

\begin{figure}[H]
    \begin{tikzpicture}
    \begin{axis}[
        title={Performance in correlation to scene complexity},
        xlabel={Triangles},
        ylabel={FPS},
        legend pos=north east,
        ymajorgrids=true,
        grid style=dashed,
        smooth,
        nodes near coords,
        x label style={below=5mm},
        x tick label style={align=center, rotate=45},
    ]
     
    \addplot+[
        color=blue,
        ]
        coordinates {
            (56,26)
            (116,19.5)
            (356,8.9)
            (1316,2.8)
            (5156,0.73)
        };
    \end{axis}
    \end{tikzpicture}
    \centering
    \caption{Scene complexity benchmark on an NVIDIA Geforce GTX 570}
    \label{fig:benchmark-scene-complexity}
\end{figure}

lol \ref{fig:benchmark-scene-complexity}

\section{Evaluation}
In order to properly rate the results seen above, they need to be put into perspective. Performance
of \acp{cpu} and \acp{gpu} varies enormously depending on their year of release due to technical
progress.
\begin{tikzpicture}
\begin{axis}[
    title={GFLOPS of tested graphics cards by year of release},
    xlabel={Year of release},
    ylabel={Peak single precision performance [GFLOPS]},
    xmin=2008, xmax=2016,
    xtick={2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016},
    ytick={100, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    nodes near coords,
    x label style={below=5mm},
    x tick label style={align=center, rotate=45},
    /pgf/number format/.cd,
    use comma,
    1000 sep={}
]
 
\addplot+[
    color=blue,
    only marks,
    point meta=explicit symbolic
    ]
    coordinates {
        (2010,1405) [\tiny NVIDIA Geforce GTX 570]
        (2013,5046) [\tiny NVIDIA Geforce GTX 780 Ti]
        (2013,4848) [\tiny AMD R9 290]
        (2014,3494) [\tiny NVIDIA Geforce GTX 970]
    };
    \legend{GPUs}
\end{axis}
\end{tikzpicture}

All \ac{gflops} numbers are taken from the chip designers' official sites.

It should be noted that while \ac{gflops} isn't the only relevant criterion to estimating the
performance of a \ac{gpu}, it's likely the most important one.

\chapter{Conclusion}

\section{Outlook}
Scene lookup performance could be drastically improved by employing an acceleration structure such
as BVH or kd-trees.

Convergence performance could be best improved by making use of bidirectional path tracing and
better sampling techniques such as \textit{Multiple Importance Sampling}.

The implementation's usage of memory architectures is likely far from optimal and could be
improved.

Lastly, overall image quality could be improved by running an image filter over the rendered image
before displaying it to the user. A suitable image filtering algorithm should be non-linear and
edge-preserving as well as energy-preserving so that it won't lower the quality of the resulting
image. A notable filter fulfilling those requirements is the \textit{bilateral filter}. Popular but
unsuitable algorithms include the \textit{box filter} and the \textit{gaussian filter}.

\listoffigures
 
\listoftables

\bibliographystyle{unsrt}
\bibliography{main}

\chapter*{Eidesstattliche Erklärung}
\onehalfspace
„Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit im
Studiengang Informatik selbstständig verfasst und keine anderen als die
angegebenen Hilfsmittel – insbesondere keine im Quellenverzeichnis nicht
benannten Internet-Quellen – benutzt habe. Alle Stellen, die wörtlich oder
sinngemäß aus Veröffentlichungen entnommen wurden, sind als solche kenntlich
gemacht. Ich versichere weiterhin, dass ich die Arbeit vorher nicht in einem
anderen Prüfungsverfahren eingereicht habe und die eingereichte schriftliche
Fassung der auf dem elektronischen Speichermedium entspricht.“
\singlespace
    
\end{document}
